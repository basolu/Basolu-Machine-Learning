{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "user_verification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrRy1/Ks0SSZANFJ5eHVJ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basolu/Basolu-Machine-Learning/blob/main/user_verification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "TD2-JHXB6CVM",
        "outputId": "b980eb93-dbe7-44d2-985c-f4cbc3d1bd38"
      },
      "source": [
        "import tensorflow as tf\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
        "from numpy import save\n",
        "from math import*\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "action = int(input(\"Run training (yes-1 \\ no-0)?\"))\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + group + '/Inertial Signals/'\n",
        "  # load all 9 files as a single array\n",
        "  filenames = list()\n",
        "  # total acceleration\n",
        "  filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "  # body acceleration\n",
        "  filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "  # body gyroscope\n",
        "  filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "  return X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "  # load all train\n",
        "  trainX, trainy = load_dataset_group('train', prefix + 'drive/MyDrive/HARDataset/')\n",
        "  #print(\"Shape trainX and trainy:\", trainX.shape, trainy.shape)\n",
        "  # load all test\n",
        "  testX, testy = load_dataset_group('test', prefix + 'drive/MyDrive/HARDataset/')\n",
        "  subjects = read_csv('drive/MyDrive/HARDataset/train/subject_train.txt', header=None, delim_whitespace=True)\n",
        "  # zero-offset class values\n",
        "  trainy = trainy - 1\n",
        "  testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "  #print(\"Shape train and test:\",trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  #print(trainX.shape[1], trainX.shape[2], trainy.shape[1]) \n",
        "  return trainX, trainy, testX, testy\n",
        "\n",
        "def autoencoder_model(X):\n",
        "  trainX = X\n",
        "  model = Sequential(\n",
        "    [\n",
        "      layers.Input(shape=(trainX.shape[1], trainX.shape[2])),\n",
        "      layers.Conv1D(\n",
        "          filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "      layers.Dropout(rate=0.2),\n",
        "      layers.Conv1D(\n",
        "          filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "      layers.Conv1DTranspose(\n",
        "          filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "      layers.Dropout(rate=0.2),\n",
        "      layers.Conv1DTranspose(\n",
        "          filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
        "      layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
        "    ]\n",
        ")\n",
        "  return model\n",
        "\n",
        "#training-set for user 1's walking\n",
        "def fun_target_train():\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  subject = load_file(\"drive/MyDrive/HARDataset/train/subject_train.txt\")\n",
        "  #training only user n°2's data\n",
        "  train_data = np.ndarray([])\n",
        "  #train_target = np.ndarray([])\n",
        "  size = 0\n",
        "  for i in range(0, len(subject)):\n",
        "      if (subject[i] == 2 and trainy[i][0]):\n",
        "        size +=1\n",
        "  train_data = np.empty([size, trainX.shape[1], trainX.shape[2]])\n",
        "  #train_target = np.empty([size, trainy.shape[1]])\n",
        "  count = 0\n",
        "  for i in range(0, len(subject)):\n",
        "      if (subject[i] == 2 and trainy[i][0]): \n",
        "        train_data[count] = trainX[i]\n",
        "        #train_target[count] = trainy[count]\n",
        "        count += 1 \n",
        "  print(\"Number of train data:\", size)\n",
        "  np.save('drive/MyDrive/HARDataset/train_data.npy', train_data)\n",
        "  #np.save('drive/MyDrive/HARDataset/train_target.npy', train_target)\n",
        "  return train_data#train_target\n",
        "\n",
        "#testing-set for user 2's walking\n",
        "def fun_target_test():\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  subject = load_file(\"drive/MyDrive/HARDataset/test/subject_test.txt\")\n",
        "  print(subject)\n",
        "  #training only user n°2's data\n",
        "  test_data = np.ndarray([])\n",
        "  size = 0\n",
        "  for i in range(0, len(subject)):\n",
        "      if (subject[i] == 2 and testy[i][0]):\n",
        "        size +=1\n",
        "  test_data = np.empty([size, testX.shape[1], testX.shape[2]])\n",
        "  count = 0\n",
        "  for i in range(0, len(subject)):\n",
        "      if (subject[i] == 2 and testy[i][0]): \n",
        "        test_data[count] = testX[i]\n",
        "        count += 1\n",
        "  print(\"Number of test data:\", size)\n",
        "  np.save('drive/MyDrive/HARDataset/test_data.npy', test_data)\n",
        "  return test_data\n",
        "\n",
        "def run_training():\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  train_data = fun_target_train()\n",
        "  test_data = fun_target_test()\n",
        "  model = autoencoder_model(train_data)\n",
        "  model.compile(optimizer='adam', loss='mae')\n",
        "  #model.summary()\n",
        "  nb_epochs = 50\n",
        "  batch_size = 10\n",
        "  history = model.fit(train_data, train_data, epochs=nb_epochs, batch_size=batch_size, validation_split=0.05).history\n",
        "  print(\"Training completed\")\n",
        "  #save the trained model\n",
        "  model_json = model.to_json()\n",
        "  with open(\"drive/MyDrive/HARDataset/model_target.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "  #serialize weights to HDF5\n",
        "  model.save_weights(\"drive/MyDrive/HARDataset/model_target.h5\")\n",
        "  print(\"Saved model to disk\")\n",
        "\n",
        "def detect_anomalies():\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  train_data = np.load('drive/MyDrive/HARDataset/train_data.npy')\n",
        "  test_data  = np.load('drive/MyDrive/HARDataset/test_data.npy')\n",
        "  print(\"Dataset loaded\")\n",
        "  json_file = open(\"drive/MyDrive/HARDataset/model_HV.json\", \"r\")\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  #load model from a json model\n",
        "  model = model_from_json(loaded_model_json)\n",
        "  # load weights into new model\n",
        "  model.load_weights(\"drive/MyDrive/HARDataset/model_HV.h5\")\n",
        "  print(\"Loaded model from disk\")  \n",
        "  \n",
        "  train_data_pred = model.predict(train_data)\n",
        "  train_mae_loss = np.mean(np.abs(train_data_pred - train_data), axis=1)\n",
        "  train_mae_loss = train_mae_loss.reshape((-1))\n",
        "  plt.hist(train_mae_loss, bins=50)\n",
        "  plt.xlabel(\"test MAE loss\")\n",
        "  plt.ylabel(\"No of samples\")\n",
        "  plt.show()\n",
        "  # Detect all the samples which are anomalies.\n",
        "  threshold = np.max(train_mae_loss)\n",
        "  anomalies = train_mae_loss > threshold\n",
        "  print(\"Reconstruction error threshold: \", threshold)\n",
        "  print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
        "  print(\"Indices of anomaly samples: \", np.where(anomalies))\n",
        "\n",
        "  test_data_pred = model.predict(test_data)\n",
        "  test_mae_loss = np.mean(np.abs(test_data_pred - test_data), axis=1)\n",
        "  test_mae_loss = test_mae_loss.reshape((-1))\n",
        "  threshold_test = np.max(test_mae_loss)\n",
        "\n",
        "  plt.hist(test_mae_loss, bins=50)\n",
        "  plt.xlabel(\"test MAE loss\")\n",
        "  plt.ylabel(\"No of samples\")\n",
        "  plt.show()\n",
        "\n",
        "  # Detect all the samples which are anomalies.\n",
        "  anomalies = test_mae_loss > threshold_test\n",
        "  print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
        "  print(\"Indices of anomaly samples: \", np.where(anomalies))\n",
        "\n",
        "  # Checking how the first sequence is learnt\n",
        "\n",
        "  plt.plot(trainX[0])\n",
        "  plt.plot(trainX_pred[0])\n",
        "  plt.show()\n",
        "\n",
        "if (action == 1):\n",
        "  run_training()\n",
        "detect_anomalies()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run training (yes-1 \\ no-0)?1\n",
            "Number of train data: 0\n",
            "[[ 2]\n",
            " [ 2]\n",
            " [ 2]\n",
            " ...\n",
            " [24]\n",
            " [24]\n",
            " [24]]\n",
            "Number of test data: 59\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-4be7eb15e1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m   \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0mdetect_anomalies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-4be7eb15e1b8>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m   \u001b[0mnb_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m#save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1118\u001b[0m       (x, y, sample_weight), validation_data = (\n\u001b[1;32m   1119\u001b[0m           data_adapter.train_validation_split(\n\u001b[0;32m-> 1120\u001b[0;31m               (x, y, sample_weight), validation_split=validation_split))\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0;34m\"`validation_split={validation_split}`. Either provide more data, or a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \"different value for the `validation_split` argument.\" .format(\n\u001b[0;32m-> 1481\u001b[0;31m             batch_dim=batch_dim, validation_split=validation_split))\n\u001b[0m\u001b[1;32m   1482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.05`. Either provide more data, or a different value for the `validation_split` argument."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmKIxB2l_eJX",
        "outputId": "51c1c86f-e77e-432d-f5a0-b1ce85d657cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5jQTvqXGcD9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}