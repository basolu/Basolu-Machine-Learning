{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.pynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdHLYAF6xPsz4B1OsJHMAn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basolu/Basolu-Machine-Learning/blob/main/main_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQGnXJpLjdkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81a7c20-ee00-404a-f426-7b4ad071a47e"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "import os\n",
        "from numpy import save\n",
        "from math import*\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Model\n",
        "\n",
        "train = int(input(\"Run training? (1/0): \"))\n",
        "print(train)\n",
        "\n",
        "def euclidean_distance(x,y):\n",
        "    return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
        "\n",
        "def manhattan_distance(x,y):\n",
        "    return sum(abs(a-b) for a,b in zip(x,y))\n",
        "\n",
        "def nth_root(value, n_root): \n",
        "    root_value = 1/float(n_root)\n",
        "    return round (Decimal(value) ** Decimal(root_value),3)\n",
        " \n",
        "def minkowski_distance(x,y,p_value):\n",
        "    return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value)\n",
        "\n",
        "def square_rooted(x):\n",
        "    return round(sqrt(sum([a*a for a in x])),3)\n",
        " \n",
        "def cosine_similarity(x,y):\n",
        "    numerator = sum(a*b for a,b in zip(x,y))\n",
        "    denominator = square_rooted(x)*square_rooted(y)\n",
        "    return round(numerator/float(denominator),3)\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + group + '/Inertial Signals/'\n",
        "  # load all 9 files as a single array\n",
        "  filenames = list()\n",
        "  # total acceleration\n",
        "  filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "  # body acceleration\n",
        "  filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "  # body gyroscope\n",
        "  filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "  return X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "  # load all train\n",
        "  trainX, trainy = load_dataset_group('train', prefix + 'drive/MyDrive/HARDataset/')\n",
        "  print(trainX.shape, trainy.shape)\n",
        "  # load all test\n",
        "  testX, testy = load_dataset_group('test', prefix + 'drive/MyDrive/HARDataset/')\n",
        "  subjects = read_csv('drive/MyDrive/HARDataset/train/subject_train.txt', header=None, delim_whitespace=True)\n",
        "  print(testX.shape, testy.shape)\n",
        "  # zero-offset class values\n",
        "  trainy = trainy - 1\n",
        "  testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "  print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  return trainX, trainy, testX, testy\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 1, 15, 32\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  #classificazione delle features estratte: \n",
        "  model.add(Flatten())\n",
        "  #numero di neuroni (100) = numero di classi delle attivitÃ  da individuare\n",
        "  model.add(Dense(100, activation='relu')) \n",
        "  extract = Model(model.inputs, model.layers[-3].output)\n",
        "  features = extract.predict(testX)\n",
        "  save('features_marco.txt', features)\n",
        "  model.add(Dense(n_outputs, activation='softmax')) \n",
        "  tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[tensorboard], validation_data=(testX, testy))\n",
        "  model_json = model.to_json()\n",
        "  with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(\"model.h5\")\n",
        "  print(\"Saved model to disk\")\n",
        "  # evaluate model\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores)\n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "def new_classification_report(loaded_model, testX, testy, features):\n",
        "  print(\"Start classification report\")\n",
        "  features_print = read_csv('drive/MyDrive/HARDataset/activity_labels.txt', header=None, delim_whitespace=True)\n",
        "  testX = testX.reshape(len(testX),128,9)\n",
        "  result = (loaded_model.predict(testX))\n",
        "  list_result = list()\n",
        "  list_test = list()\n",
        "  columns = 6\n",
        "  for r in range(0,len(testy)): \n",
        "    index = 0\n",
        "    max = 0\n",
        "    for t in range(0,columns):\n",
        "      prov = float(result[r][t])\n",
        "      if(prov > max):\n",
        "        max = prov\n",
        "        index = t\n",
        "      if(testy[r][t] == 1):\n",
        "        list_test.append(t+1)\n",
        "    list_result.append(index+1)\n",
        "  print(classification_report(list_test, list_result, target_names=features_print[1]))\n",
        "  print(\"End classification_report\")\n",
        "\n",
        "def new_predict(loaded_model, testX, testy):\n",
        "  print(\"Start predict\")\n",
        "  features_print = read_csv('drive/MyDrive/HARDataset/activity_labels.txt', header=None, delim_whitespace=True)\n",
        "  request = 1\n",
        "  while(request > 0):\n",
        "    request = int(input(\"Inserire l'indice della misurazione da riconoscere (0 per uscire): \"))\n",
        "    target = testX[request]\n",
        "    target = target.reshape(1,128,9)\n",
        "    testX = testX.reshape(len(testX),128,9)\n",
        "    result = (loaded_model.predict(target))\n",
        "    #result = result.reshape(len(testX),6,1)\n",
        "    columns = 6\n",
        "    max = 0\n",
        "    index = 0\n",
        "    for t in range(0,columns):\n",
        "      prov = result[0][t]\n",
        "      if(prov > max):\n",
        "        max = prov\n",
        "        index = t\n",
        "    print(request, end=' ')\n",
        "    print(features_print[1][index], end=' ')\n",
        "    print(\"({})\" .format(index+1))      \n",
        "    print(\"Correct: \", testy[request])\n",
        "  print(\"Fine prediction\")\n",
        "\n",
        "def new_evaluate_fdistance(loaded_model, testX, testy, features):\n",
        "  print(\"Start evaluation of the features distances\")\n",
        "  features_print = read_csv('drive/MyDrive/HARDataset/activity_labels.txt', header=None, delim_whitespace=True)\n",
        " # print(\"Summary\", loaded_model.summary())\n",
        "  extract = Model(loaded_model.inputs, loaded_model.layers[-2].output)\n",
        "  features = extract.predict(testX)\n",
        "  print(features.shape)\n",
        "  #features = features.reshape(features.shape[0], (features.shape[1] * features.shape[2]))\n",
        "  #print(\"Features: \", features[r])\n",
        "  #print(\"Features shape: \", features.shape) #(2947, 3968) --> 3968=128x31 con 31 nÂ° di soggeti\n",
        "  '''print(\"Start distance calculation (input -1 to end)\")\n",
        "  x = ddd\n",
        "  while(1):\n",
        "    x = int(input(\"Firts features: \"))\n",
        "    if (x == -1): return\n",
        "    y = int(input(\"Second features: \"))\n",
        "    print(\"Euclide:\",euclidean_distance(features[x], features[y]))\n",
        "    print(\"Manhattan:\",manhattan_distance(features[x], features[y]))\n",
        "    print(\"Cosine:\", cosine_similarity(features[x], features[y]))\n",
        "  '''\n",
        "  tot_corrects = 0\n",
        "  find_corrects = 0\n",
        "  cos_prov = 0\n",
        "  for r in range(0, (features.shape[0]-1)):\n",
        "    for t in range(0, (features.shape[0]-1)):\n",
        "      if (sum(testy[r]*testy[t])):\n",
        "        tot_corrects += 1\n",
        "        cos_prov = cosine_similarity(features[r], features[t])\n",
        "        if (cos_prov >= 0.8): find_corrects += 1\n",
        "  print(\"Total corrects:\", tot_corrects)\n",
        "  print(\"Corrects found:\", find_corrects)\n",
        "  print(\"% accuracy (with cos>0.8):\", (find_corrects/tot_corrects)*100)\n",
        "\n",
        "  #print(sum(testy[6]*testy[80]))\n",
        "\n",
        "  '''layers[-2] sembra essere quello piÃ¹ accurato, un po troppo \"ottimista\",\n",
        "    attivitÃ  completamente diverse di soggetti diversi restituiscono una \n",
        "    distanza cosenica mai inferiore allo 0,3\n",
        "    Anche il layer[-5] risulta abbastanza accurato, diminuendo la precisone\n",
        "    quando si analizzano le features di livello piÃ¹ alto\n",
        "  '''\n",
        "\n",
        "def predict_prova(): \n",
        "  mode = int(input(\"Classification_report(2), Evaluate_fdistance(1) or predict(0)? \"))\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  features_print = read_csv('drive/MyDrive/HARDataset/activity_labels.txt', header=None, delim_whitespace=True)\n",
        "  json_file = open('drive/MyDrive/HARDataset/model.json', 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  loaded_model = model_from_json(loaded_model_json)\n",
        "  # load weights into new model\n",
        "  loaded_model.load_weights(\"drive/MyDrive/HARDataset/model.h5\")\n",
        "  print(\"Loaded model from disk\")\n",
        "  loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "  _, accuracy = loaded_model.evaluate(testX, testy, batch_size=32, verbose=1)\n",
        "  print(\"Accuracy: \",accuracy)\n",
        "  extract = Model(loaded_model.inputs, loaded_model.layers[-3].output)\n",
        "  features = extract.predict(testX)\n",
        "  if (mode == 2): \n",
        "    new_classification_report(loaded_model, testX, testy, features)  \n",
        "  elif (mode == 1):\n",
        "    print(\"Start \")\n",
        "    new_evaluate_fdistance(loaded_model, testX, testy, features)\n",
        "  elif (mode == 0):\n",
        "    #effetua la predict delle misurazioni, riconoscendo l'azione relativa, \n",
        "    #tramite gli indici di tali misurazioni\n",
        "    new_predict(loaded_model, testX, testy)\n",
        "  \n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=10):\n",
        "  print(\"Start experiment\")\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  # repeat experiment\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "\t  # summarize results\n",
        "    summarize_results(scores)\n",
        "\n",
        "# run the experiment\n",
        "if(train == 1):\n",
        "   run_experiment()\n",
        "predict_prova()\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run training? (1/0): 0\n",
            "0\n",
            "Classification_report(2), Evaluate_fdistance(1) or predict(0)? 1\n",
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            "Loaded model from disk\n",
            "93/93 [==============================] - 2s 7ms/step - loss: 0.8088 - accuracy: 0.8727\n",
            "Accuracy:  0.8995589017868042\n",
            "Start \n",
            "Start evaluation of the features distances\n",
            "(2947, 100)\n",
            "Total corrects: 1455790\n",
            "Corrects found: 1250190\n",
            "% accuracy (with cos>0.8): 85.87708392007089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8dW8y2J2Htt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a4ba8b-216e-4990-a08a-2c92ece5eca7"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7-Zhhc-Lagy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b_WgJFFyXg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63ab1fcb-362b-43e0-a6fe-9f668e1158b5"
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow-gpu"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/fd/993aa1333eb54d9f000863fe8ec61e41d12eb833dea51484c76c038718b5/tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 454.3MB 26kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/78/b27f73e923becc6e79e18fe112cf75e3200d1ee35b0dba8fa46181bce56c/tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 471kB 27.3MB/s \n",
            "\u001b[?25hCollecting gast==0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Collecting tensorboard~=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 6.0MB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.1.0\n",
            "    Uninstalling tensorflow-estimator-2.1.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "  Found existing installation: tensorboard 2.1.1\n",
            "    Uninstalling tensorboard-2.1.1:\n",
            "      Successfully uninstalled tensorboard-2.1.1\n",
            "  Found existing installation: tensorflow 2.1.0\n",
            "    Uninstalling tensorflow-2.1.0:\n",
            "      Successfully uninstalled tensorflow-2.1.0\n",
            "Successfully installed gast-0.4.0 tensorboard-2.5.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/a2/5ccf0a418eb22e0a2ae9edc1e7f5456d0a4b8b49524572897564b4030a9b/tensorflow_gpu-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 454.3MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.34.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (3.4.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTAoVnsgLa6Q"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}