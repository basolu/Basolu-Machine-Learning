{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.pynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMdvngKdJ5T9GIH9VBc+g99",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basolu/Basolu-Machine-Learning/blob/main/main_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQGnXJpLjdkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21413a9d-a646-42e0-be80-0fb48f6aa5b8"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "import os\n",
        "from numpy import save\n",
        "from math import*\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Model\n",
        "\n",
        "train = int(input(\"Run training? (1/0): \"))\n",
        "print(train)\n",
        "\n",
        "def euclidean_distance(x,y):\n",
        "    return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
        "\n",
        "def manhattan_distance(x,y):\n",
        " \n",
        "    return sum(abs(a-b) for a,b in zip(x,y))\n",
        "\n",
        "def jaccard_similarity(x,y):\n",
        "\n",
        "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
        "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
        "    return intersection_cardinality/float(union_cardinality)\n",
        "\n",
        "def nth_root(value, n_root):\n",
        " \n",
        "    root_value = 1/float(n_root)\n",
        "    return round (Decimal(value) ** Decimal(root_value),3)\n",
        " \n",
        "def minkowski_distance(x,y,p_value):\n",
        " \n",
        "    return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value)\n",
        "\n",
        "def square_rooted(x):\n",
        " \n",
        "    return round(sqrt(sum([a*a for a in x])),3)\n",
        " \n",
        "def cosine_similarity(x,y):\n",
        " \n",
        "    numerator = sum(a*b for a,b in zip(x,y))\n",
        "    denominator = square_rooted(x)*square_rooted(y)\n",
        "    return round(numerator/float(denominator),3)\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + group + '/Inertial Signals/'\n",
        "  # load all 9 files as a single array\n",
        "  filenames = list()\n",
        "  # total acceleration\n",
        "  filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "  # body acceleration\n",
        "  filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "  # body gyroscope\n",
        "  filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "  return X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "  # load all train\n",
        "  trainX, trainy = load_dataset_group('train', prefix + 'drive/MyDrive/HARDataset/')\n",
        "  print(trainX.shape, trainy.shape)\n",
        "  # load all test\n",
        "  testX, testy = load_dataset_group('test', prefix + 'drive/MyDrive/HARDataset/')\n",
        "  subjects = read_csv('drive/MyDrive/HARDataset/train/subject_train.txt', header=None, delim_whitespace=True)\n",
        "  print(testX.shape, testy.shape)\n",
        "  # zero-offset class values\n",
        "  trainy = trainy - 1\n",
        "  testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "  print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  return trainX, trainy, testX, testy\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 1, 15, 32\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  #classificazione delle features estratte: \n",
        "  model.add(Flatten())\n",
        "  #numero di neuroni (100) = numero di classi delle attività da individuare\n",
        "  model.add(Dense(100, activation='relu')) \n",
        "  extract = Model(model.inputs, model.layers[-3].output)\n",
        "  features = extract.predict(testX)\n",
        "  save('features_marco.txt', features)\n",
        "  model.add(Dense(n_outputs, activation='softmax')) \n",
        "  tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[tensorboard], validation_data=(testX, testy))\n",
        "  model_json = model.to_json()\n",
        "  with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(\"model.h5\")\n",
        "  print(\"Saved model to disk\")\n",
        "  # evaluate model\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores)\n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "def predict_prova(): \n",
        "  mode = int(input(\"Classification_report(1) or predict(0)? \"))\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  features_print = read_csv('drive/MyDrive/HARDataset/activity_labels.txt', header=None, delim_whitespace=True)\n",
        "  json_file = open('drive/MyDrive/HARDataset/model.json', 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  loaded_model = model_from_json(loaded_model_json)\n",
        "  # load weights into new model\n",
        "  loaded_model.load_weights(\"drive/MyDrive/HARDataset/model.h5\")\n",
        "  print(\"Loaded model from disk\")\n",
        "  loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "  extract = Model(loaded_model.inputs, loaded_model.layers[-3].output)\n",
        "  features = extract.predict(testX)\n",
        "  #print(\"Features: \", features[r])\n",
        "  #print(\"Features shape: \", features.shape) #(2947, 3968) --> 3968=128x31 con 31 n° di soggeti\n",
        "  _, accuracy = loaded_model.evaluate(testX, testy, batch_size=32, verbose=1)\n",
        "  print(\"Accuracy: \",accuracy)\n",
        "  columns = 6\n",
        "  if(mode == 1):\n",
        "    print(\"Start classification report\")\n",
        "    testX = testX.reshape(len(testX),128,9)\n",
        "    result = (loaded_model.predict(testX))\n",
        "    list_result = list()\n",
        "    list_test = list()\n",
        "    #Questa parte della funzione stampa la relativa features di tutti i sample\n",
        "    for r in range(0,len(testy)): \n",
        "      index = 0\n",
        "      max = 0\n",
        "      for t in range(0,columns):\n",
        "        prov = float(result[r][t])\n",
        "        if(prov > max):\n",
        "          max = prov\n",
        "          index = t\n",
        "        if(testy[r][t] == 1):\n",
        "          list_test.append(t+1)\n",
        "      list_result.append(index+1)\n",
        "      #print(r+1, end=' ')\n",
        "      #print(index+1, end=' ')\n",
        "      #print(features[1][index])\n",
        "    print(classification_report(list_test, list_result, target_names=features_print[1]))\n",
        "    #print(\"Summary:\", loaded_model.summary(line_length=None, positions=None, print_fn=None))\n",
        "    print(\"End classification_report\")\n",
        "    print(\"Start distance calculation (input -1 to end)\")\n",
        "    x = 0\n",
        "    while(x != -1):\n",
        "      x = int(input(\"Firts features: \"))\n",
        "      y = int(input(\"Second features: \"))\n",
        "      print(\"Euclide:\",euclidean_distance(features[x], features[y]))\n",
        "      print(\"Manhattan:\",manhattan_distance(features[x], features[y]))\n",
        "      print(\"Cosine:\", cosine_similarity(features[x], features[y]))\n",
        "\n",
        "  else:\n",
        "    print(\"Start predict\")\n",
        "    request = 1\n",
        "    while(request > 0):\n",
        "      request = int(input(\"Inserire l'indice della misurazione da riconoscere (0 per uscire): \"))\n",
        "      target = testX[request]\n",
        "      target = target.reshape(1,128,9)\n",
        "      testX = testX.reshape(len(testX),128,9)\n",
        "      result = (loaded_model.predict(target))\n",
        "      #result = result.reshape(len(testX),6,1)\n",
        "      columns = 6\n",
        "      max = 0\n",
        "      index = 0\n",
        "      for t in range(0,columns):\n",
        "        prov = result[0][t]\n",
        "        if(prov > max):\n",
        "          max = prov\n",
        "          index = t\n",
        "      print(request, end=' ')\n",
        "      print(features[1][index], end=' ')\n",
        "      print(\"({})\" .format(index+1))\n",
        "      \n",
        "      print(\"Correct: \", testy[request])\n",
        "    print(\"Fine prediction\")\n",
        "      \n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=10):\n",
        "  print(\"Start experiment\")\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  # repeat experiment\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "\t  # summarize results\n",
        "    summarize_results(scores)\n",
        "\n",
        "# run the experiment\n",
        "if(train == 1):\n",
        "   run_experiment()\n",
        "predict_prova()\n",
        "\n",
        "#identificazione: riconoscimento del soggetto all'interno di un set di utenti\n",
        "#autenticazione: verifica che un dato sample, sia appartenente a quella determinata persona ***\n",
        "#addestrare per riconoscere l'attività, e poi per distinguere gli indiivdui andiamo ad estrarre le features e confrontate\n",
        "#con tecniche quali la verifica della loro distanza "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run training? (1/0): 0\n",
            "0\n",
            "Classification_report(1) or predict(0)? 1\n",
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            "Loaded model from disk\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.8088 - accuracy: 0.8727\n",
            "Accuracy:  0.8995589017868042\n",
            "Start classification report\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "           WALKING       1.00      0.88      0.94       496\n",
            "  WALKING_UPSTAIRS       0.89      0.94      0.91       471\n",
            "WALKING_DOWNSTAIRS       0.87      0.96      0.91       420\n",
            "           SITTING       0.88      0.70      0.78       491\n",
            "          STANDING       0.80      0.91      0.85       532\n",
            "            LAYING       0.99      1.00      0.99       537\n",
            "\n",
            "          accuracy                           0.90      2947\n",
            "         macro avg       0.90      0.90      0.90      2947\n",
            "      weighted avg       0.90      0.90      0.90      2947\n",
            "\n",
            "End classification_report\n",
            "Start distance calculation (input -1 to end)\n",
            "Firts features: 5\n",
            "Second features: 6\n",
            "Euclide: 0.36041646050440673\n",
            "Manhattan: 8.74804368102923\n",
            "Cosine: 0.995\n",
            "Firts features: -1\n",
            "Second features: 2\n",
            "Euclide: 6.295257174398247\n",
            "Manhattan: 196.70234774949495\n",
            "Cosine: 0.407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8dW8y2J2Htt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bbc2961-3344-4f7c-f36b-755e9dbd1be6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7-Zhhc-Lagy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTAoVnsgLa6Q"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}