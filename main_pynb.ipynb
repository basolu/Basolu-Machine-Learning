{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.pynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOiTJiIympjhTdbiC0SyDgq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basolu/Basolu-Machine-Learning/blob/main/main_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQGnXJpLjdkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d7afa5-bcac-4e9d-9b76-586f23905764"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "train = int(input(\"Run training? (1/0): \"))\n",
        "print(train)\n",
        "\n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "\tdataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        "\treturn dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "\tloaded = list()\n",
        "\tfor name in filenames:\n",
        "\t\tdata = load_file(prefix + name)\n",
        "\t\tloaded.append(data)\n",
        "\t# stack group so that features are the 3rd dimension\n",
        "\tloaded = dstack(loaded)\n",
        "\treturn loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + group + '/Inertial Signals/'\n",
        "  # load all 9 files as a single array\n",
        "  filenames = list()\n",
        "  # total acceleration\n",
        "  filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "  # body acceleration\n",
        "  filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "  # body gyroscope\n",
        "  filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "  return X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "  # load all train\n",
        "  trainX, trainy = load_dataset_group('train', prefix + 'drive/MyDrive/HARDataset/')\n",
        "  print(trainX.shape, trainy.shape)\n",
        "  # load all test\n",
        "  testX, testy = load_dataset_group('test', prefix + 'drive/MyDrive/HARDataset/')\n",
        "  subjects = read_csv('drive/MyDrive/HARDataset/train/subject_train.txt', header=None, delim_whitespace=True)\n",
        "  print(testX.shape, testy.shape)\n",
        "  # zero-offset class values\n",
        "  trainy = trainy - 1\n",
        "  testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "  print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "  return trainX, trainy, testX, testy\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 1, 15, 32\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  \n",
        "  model = Sequential()\n",
        "  #estrazione featuers\n",
        "  #primo layer convoluzionale: applica le varie features per classificare\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  #classificazione delle features estratte: \n",
        "  model.add(Flatten())\n",
        "  #numero di neuroni (100) = numero di classi delle attività da individuare\n",
        "  model.add(Dense(100, activation='relu')) \n",
        "  #n_output = number of subjects -> da impostare col n° delle attività\n",
        "  #nell'ultimo layer il n° di neuroni dev'essere uguale al n° di etichette\n",
        "  model.add(Dense(n_outputs, activation='softmax')) \n",
        "  tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[tensorboard], validation_data=(testX, testy)\n",
        "  )\n",
        "  model_json = model.to_json()\n",
        "  with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(\"model.h5\")\n",
        "  print(\"Saved model to disk\")\n",
        "  # evaluate model\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy\n",
        "\n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores)\n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        "\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "def predict_prova(): \n",
        "  mode = int(input(\"Classification_report(1) or predict(0)? \"))\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  features = read_csv('drive/MyDrive/HARDataset/activity_labels.txt', header=None, delim_whitespace=True)\n",
        "  json_file = open('model.json', 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  loaded_model = model_from_json(loaded_model_json)\n",
        "  # load weights into new model\n",
        "  loaded_model.load_weights(\"model.h5\")\n",
        "  print(\"Loaded model from disk\")\n",
        "  loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "  _, accuracy = loaded_model.evaluate(testX, testy, batch_size=32, verbose=1)\n",
        "  print(\"Accuracy: \",accuracy)\n",
        "  columns = 6\n",
        "  if(mode == 1):\n",
        "    print(\"Start classification report\")\n",
        "    testX = testX.reshape(len(testX),128,9)\n",
        "    result = (loaded_model.predict(testX))\n",
        "    list_result = list()\n",
        "    list_test = list()\n",
        "    #Questa parte della funzione stampa la relativa features di tutti i sample\n",
        "    for r in range(0,len(testy)): \n",
        "      index = 0\n",
        "      max = 0\n",
        "      for t in range(0,columns):\n",
        "        prov = float(result[r][t])\n",
        "        if(prov > max):\n",
        "          max = prov\n",
        "          index = t\n",
        "        if(testy[r][t] == 1):\n",
        "          list_test.append(t+1)\n",
        "      list_result.append(index+1)\n",
        "      #print(r+1, end=' ')\n",
        "      #print(index+1, end=' ')\n",
        "      #print(features[1][index])\n",
        "    print(classification_report(list_test, list_result, target_names=features[1]))\n",
        "    \n",
        "  else:\n",
        "    print(\"Start predict\")\n",
        "    request = 1\n",
        "    while(request > 0):\n",
        "      request = int(input(\"Inserire l'indice della misurazione da riconoscere (0 per uscire): \"))\n",
        "      target = testX[request]\n",
        "      target = target.reshape(1,128,9)\n",
        "      testX = testX.reshape(len(testX),128,9)\n",
        "      result = (loaded_model.predict(target))\n",
        "      #result = result.reshape(len(testX),6,1)\n",
        "      columns = 6\n",
        "      max = 0\n",
        "      index = 0\n",
        "      for t in range(0,columns):\n",
        "        prov = result[0][t]\n",
        "        if(prov > max):\n",
        "          max = prov\n",
        "          index = t\n",
        "      print(request, end=' ')\n",
        "      print(features[1][index], end=' ')\n",
        "      print(\"({})\" .format(index+1))\n",
        "      \n",
        "      print(\"Correct: \", testy[request])\n",
        "    print(\"Fine prediction\")\n",
        "      \n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=10):\n",
        "  print(\"Start experiment\")\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  # repeat experiment\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "\t  # summarize results\n",
        "    summarize_results(scores)\n",
        "\n",
        "# run the experiment\n",
        "if(train == 1):\n",
        "   run_experiment()\n",
        "predict_prova()\n",
        "\n",
        "#identificazione: riconoscimento del soggetto all'interno di un set di utenti\n",
        "#autenticazione: verifica che un dato sample, sia appartenente a quella determinata persona ***\n",
        "#addestrare per riconoscere l'attività, e poi per distinguere gli indiivdui andiamo ad estrarre le features e confrontate\n",
        "#con tecniche quali la verifica della loro distanza \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run training? (1/0)0\n",
            "0\n",
            "Classification_report(1) or predict(0)? 1\n",
            "(7352, 128, 9) (7352, 1)\n",
            "(2947, 128, 9) (2947, 1)\n",
            "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
            "Loaded model from disk\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.3610 - accuracy: 0.8798\n",
            "Accuracy:  0.913810670375824\n",
            "Start classification report\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "           WALKING       0.92      0.95      0.94       496\n",
            "  WALKING_UPSTAIRS       0.95      0.92      0.93       471\n",
            "WALKING_DOWNSTAIRS       0.91      1.00      0.95       420\n",
            "           SITTING       0.86      0.80      0.83       491\n",
            "          STANDING       0.84      0.88      0.86       532\n",
            "            LAYING       1.00      0.95      0.97       537\n",
            "\n",
            "          accuracy                           0.91      2947\n",
            "         macro avg       0.91      0.92      0.91      2947\n",
            "      weighted avg       0.91      0.91      0.91      2947\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8dW8y2J2Htt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95942f0-b43a-42e2-e7d6-0ba03c037bd2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7-Zhhc-Lagy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTAoVnsgLa6Q"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}